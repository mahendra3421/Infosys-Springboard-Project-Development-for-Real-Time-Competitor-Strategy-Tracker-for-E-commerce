{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "prDMqPdHkcVQ"
   },
   "outputs": [],
   "source": [
    "!pip install transformers torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393,
     "referenced_widgets": [
      "ac5fdc1b0fc94194a080151ffba42240",
      "95d10eb06b5f4b12a928994a4133a3f7",
      "66a78a4e11f5438191df31a318178a24",
      "5a1c569ccf6b41f998357c2f620a53c6",
      "9fd760d8dfa44a2eb64b0126c157b141",
      "5bc8da705bc14a8da37a2d98072c7401",
      "5f57d7e8253a47319515bd2c10eb16f1",
      "f096410355094491a94b9048f133ade7",
      "5952783aad324852b5fae4c551f1f199",
      "13c788b79d004eb197708a3eb11b768e",
      "1fa45f4e562f40d7aeb06de680a124bc",
      "d74ba52229904460ba4acd26c915ac6a",
      "a7921e6830b04365a6de247b942fb7b7",
      "54a6bf74e20e475ea3a8ec35fde16661",
      "c2e95d6dab7f494ba6b02d7818c61c28",
      "8032d16f288540349ad784b3f2c0aaf3",
      "5d3919d4c53941138f97de95659485ac",
      "3a49ec64f36c4ab7a2cbd584c66e97ee",
      "6c28e91b31894cb28e6f46bdfdfc7862",
      "640a3bc381a54a228a64e14cca9b1fa5",
      "1aa274e777274bae9fb356ab8d70e989",
      "9073b5b471fd40e5975f70ef985bf0c5",
      "5a5c142dd4754961a01d6d6267504f6e",
      "959bcd3e0daf443da4510551ac56b2f4",
      "3077f24fabff4f059e026592bcc4c251",
      "20f3c19f05db4c7d91bce16b1226e9e2",
      "ba985252f77b4ee29612152319b002f2",
      "5c0bd0044dde40798c447599da62c0c8",
      "a9884244928d46d1b2bce969f1bf85ff",
      "308c0a0948d44d419e14c13ae89e539c",
      "549dc994d6e2437ab0398aece72b3e1c",
      "b01c6d3db3f749068a9268643f7a91ae",
      "8dc7cb3579b64bea8a83471b2025dc27",
      "897e7435adff4043a9de7f3ca80005bf",
      "a4144d325a9f4872bce29ea1c19b69d5",
      "8bcb3120c80a4120b084f4f89e9fb0c9",
      "a3ea93e3ebaf45d09f7fd32e83716f6d",
      "5a9785f058bd4b92931aee611100a76e",
      "084602b0d7f7439ba7e46e9570656ee5",
      "f843c31187c7464498d09ac2e44606f3",
      "e62696d2622d44108c6c904b388272b7",
      "3a80104a8d0a45e98bf25ada02487b8e",
      "299f7aa8d55e45f49dd5597f0c8368db",
      "66cd9bc3849d4b1aac4f92719b6e2eef",
      "4bd9f4ee13c541539c442e6b3ab32869",
      "cbfe8fd9892046cebd0bf690185a4d93",
      "6322680874904482ac1b15702285d5bf",
      "af3fd17727e94366ad945fe597546b2a",
      "0ab705a6edc240b4ac0721695d262769",
      "e4f4a29d4cba42ceb6c6f20f67c1177a",
      "39abef8244d94002bb1ccf1bb909d505",
      "a0c7a58c0a7d400b8bb0c5c3f08586d3",
      "76425111fa1b4ff7aa15876ec2a7dfe0",
      "f40ddf06a4f041ba8be89a2dfc3dd089",
      "4bc812d6d0904851a0dbbb526befd0cc"
     ]
    },
    "id": "oFp5h-_bkeSz",
    "outputId": "7117a158-e257-4c4a-d6d8-9d3e0a0383dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5fdc1b0fc94194a080151ffba42240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d74ba52229904460ba4acd26c915ac6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5c142dd4754961a01d6d6267504f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897e7435adff4043a9de7f3ca80005bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd9f4ee13c541539c442e6b3ab32869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are popular machine learning methods?\n",
      "Answer: supervised learning, unsupervised learning, and reinforcement learning\n",
      "Score: 0.7955703735351562\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load an open-source transformer QA model\n",
    "qa_model = pipeline(\"question-answering\",\n",
    "                    model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "# Provide your paragraph here\n",
    "context = \"\"\"\n",
    "Machine learning is a field of artificial intelligence that uses statistical techniques\n",
    "to give computer systems the ability to learn from data. It focuses on developing\n",
    "algorithms that improve automatically through experience. Popular ML methods include\n",
    "supervised learning, unsupervised learning, and reinforcement learning.\n",
    "\"\"\"\n",
    "\n",
    "# Ask any question based on the paragraph\n",
    "question = \"What are popular machine learning methods?\"\n",
    "\n",
    "result = qa_model(question=question, context=context)\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Score:\", result['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlnXmFAq1e7l"
   },
   "source": [
    "##**Observations**\n",
    "\n",
    "These cells install the transformers library and load an open-source Question-Answering model. A text passage is provided and the model is asked to answer a factual question from it. The model returns an answer along with a confidence score. In this case, the first question is answered correctly because the answer exists within the passage and the model is able to extract it. The confidence score indicates how sure the model is about the extracted answer, which helps in understanding the reliability of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v-IvxK0_koGD",
    "outputId": "ae879d91-8c46-43fe-93ed-82aadaccbf39"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What is the primary goal of Artificial Intelligence (AI) in computer systems?\n",
      "Answer: creating systems capable of performing tasks that normally require human intelligence\n",
      "Score: 0.11062994599342346\n",
      "\n",
      "Question 2: What is the capital of united states?\n",
      "Answer: AI\n",
      "Score: 0.3035014271736145\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load an open-source transformer QA model\n",
    "qa_model = pipeline(\"question-answering\",\n",
    "                    model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "# Provide your paragraph here\n",
    "context = \"\"\"\n",
    "Artificial Intelligence (AI) is a broad field of computer science that focuses on\n",
    "creating systems capable of performing tasks that normally require human intelligence.\n",
    "These tasks include understanding language, recognizing patterns, solving problems,\n",
    "making decisions, and learning from experience.\n",
    "AI aims to build machines that can mimic cognitive functions such as reasoning,\n",
    "perception, and decision-making.\n",
    "\"\"\"\n",
    "\n",
    "# Ask any qusetion from the paragraph\n",
    "question1 = \"What is the primary goal of Artificial Intelligence (AI) in computer systems?\"\n",
    "question2 = \"What is the capital of united states?\"\n",
    "\n",
    "result1 = qa_model(question=question1, context=context)\n",
    "\n",
    "print(\"Question 1:\", question1)\n",
    "print(\"Answer:\", result1['answer'])\n",
    "print(\"Score:\", result1['score'])\n",
    "\n",
    "result2 = qa_model(question=question2, context=context)\n",
    "\n",
    "print(\"\\nQuestion 2:\", question2)\n",
    "print(\"Answer:\", result2['answer'])\n",
    "print(\"Score:\", result2['score'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "001hCvXZs-zD"
   },
   "source": [
    "###**First Question**\n",
    "The model answers the first question correctly but with a very low confidence score, showing that it is not very sure even when the answer exists in the given context.\n",
    "\n",
    "###**Second Question**\n",
    "The second question is answered incorrectly because the model can only extract answers from the provided paragraph, and since the capital of the United States is not mentioned, it still guesses from the text, proving that this model cannot handle general knowledge questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393,
     "referenced_widgets": [
      "04ef2480ec554b549a6a34985be0a99a",
      "81ea24d79d9945c68cce2c0b2624db20",
      "4ed4480e847d4d539cad8513a3baabe6",
      "38693ef911cb4331afdfbd53cbc098e9",
      "cf8fc792ebda4c148dc6241a16b8dd9b",
      "8d33416561ee43e48bb08ee4851af82e",
      "6d7dd84b3e5245cc8c4922b3ceaa71aa",
      "ad6b28c36ddf4f738ccc6e2b240011ed",
      "55425c994dc54d26bc1459bddffd867e",
      "f668d43cdf08428baa1050b523a50e96",
      "b19b9cb8429f4d7e97d8716292f2b5f2",
      "c66cebd7ede54db980d1d566ed928301",
      "35c8e21641f948a48e83ab169123cc9d",
      "131249e0e1ff4469bf686d1e4e548a98",
      "9c76ff7b6bb9469f9041a0de6f3a44dd",
      "62b4277168fc4b3e9cbddb8d0a760b25",
      "ad2fdeb40c0e4a2b8176205934f85464",
      "bf3315a3a73e4fb28e11123e519eaf27",
      "4b04d2e6c02648ed82f046f5b76e74e4",
      "c7c9ccfcbe484aa0ba0e51af7f1dc0fe",
      "2de9d3b510384b8c880322fb1594a73b",
      "8d169cf715e34875a2781ee65eb3ed5a",
      "53aba88922d843539b48020b211016c7",
      "4ab1b7e8ca1f4044b6872d21f2b54d31",
      "03cee4b1e3bb40b180c45054d6a42707",
      "329a53f564d340fb86f08aa357526582",
      "3346ef75957740c2a582231a99947193",
      "97cb2845ef354b5e9768b2d4be3a1bd1",
      "73ccf6db79ec48abafe013b55074654d",
      "633a68cca56140db91b74e00cb6dea3e",
      "c21b38afc21741aca48e5bddd5c6e174",
      "d40ba9f666e045099f4d8b1abafaebb9",
      "f60fa867c216491a88eec21097bf042b",
      "8f150ad20a874942858f79ecb775ece7",
      "3d5d53c36569415782b20d0ef56775b3",
      "e572da12fd4f4672b1e2fa653d9d0616",
      "e5ee9f376ce241ec815641ffc1f7780e",
      "68762489d1e84945a27f93f19960178e",
      "b46562db74054ce0b6792a84b85799db",
      "e74b263c35564bde8b21b66146e0b5c6",
      "7372400ce7774e179844fa2a13ee153d",
      "59b9c27080d044e890fe3eef315c7d5a",
      "12fe660d5a3246629d5459c10e0ebe08",
      "d8aba42a2a954669b3451a1ae9462025",
      "0abdbc145c694ee484f4f6e3952af6b0",
      "1407f95f7b4e4ea4b1a9dea76b54dc5b",
      "9469b15fcf7a4067a72d62a962871d77",
      "2e531a10b0bc4c34acd3cf772b33ace0",
      "4c281709140540bc98c9d250fec4fcd9",
      "0f6508d0858743159fbbfb4087f13f1c",
      "434d3c1ad355437aa7a30b876ea72daf",
      "b65d4d2ef1be452cb6874b8ab6e7b39f",
      "1a28c02b1f0e44088a5040804ca32361",
      "d7ee43529d274d29bc3c2c11cdef8dfb",
      "026ba6c0c83a442d8b694c80a317e94b"
     ]
    },
    "id": "L-z-06AQpYfX",
    "outputId": "f7b583d5-4a94-4223-d78d-2e42b23f4f77"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ef2480ec554b549a6a34985be0a99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66cebd7ede54db980d1d566ed928301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53aba88922d843539b48020b211016c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f150ad20a874942858f79ecb775ece7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abdbc145c694ee484f4f6e3952af6b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What is the primary goal of Artificial Intelligence (AI) in computer systems?\n",
      "Answer: performing tasks that normally require human intelligence\n",
      "Score: 0.1530224084854126\n",
      "\n",
      "Question 2: What is the capital of India?\n",
      "Answer: human intelligence\n",
      "Score: 0.0010961239458993077\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load an open-source transformer QA model\n",
    "qa_model = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    ")\n",
    "\n",
    "# Provide your paragraph here\n",
    "context = \"\"\"\n",
    "Artificial Intelligence (AI) is a broad field of computer science that focuses on\n",
    "creating systems capable of performing tasks that normally require human intelligence.\n",
    "These tasks include understanding language, recognizing patterns, solving problems,\n",
    "making decisions, and learning from experience.\n",
    "AI aims to build machines that can mimic cognitive functions such as reasoning,\n",
    "perception, and decision-making.\n",
    "\"\"\"\n",
    "\n",
    "# Ask any qusetion from the paragraph\n",
    "question1 = \"What is the primary goal of Artificial Intelligence (AI) in computer systems?\"\n",
    "question2 = \"What is the capital of India?\"\n",
    "\n",
    "result1 = qa_model(question=question1, context=context)\n",
    "\n",
    "print(\"Question 1:\", question1)\n",
    "print(\"Answer:\", result1['answer'])\n",
    "print(\"Score:\", result1['score'])\n",
    "\n",
    "result2 = qa_model(question=question2, context=context)\n",
    "\n",
    "print(\"\\nQuestion 2:\", question2)\n",
    "print(\"Answer:\", result2['answer'])\n",
    "print(\"Score:\", result2['score'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqnzS4XYt-gs"
   },
   "source": [
    "###**First Question**\n",
    "The first question is answered correctly with a slightly higher confidence, showing that the new model\n",
    "(bert-large-uncased-whole-word-masking-finetuned-squad) extracts information better than the previous model, even though the score is still low because the answer is not an exact match from the paragraph.\n",
    "\n",
    "###**Second Question**\n",
    "The second question is answered incorrectly, because the model can only extract answers from the given context.\n",
    "Since the paragraph does not contain the capital of the United States, the model still tries to pick some random text from the paragraph, proving that this QA model cannot answer general knowledge questions—only context-based ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370,
     "referenced_widgets": [
      "4762cc40db204d8b8664aef1e659d733",
      "7089f3f8950e423b873aeb34efc84cc5",
      "ba8f38d43946468ba86abf2df99c04ad",
      "16210274e74c4c46aaaf9f05383bb320",
      "3c2f52ad514a4654a6a3c4fb19d1a84a",
      "580cada9547948e0abd9e2b5f87b147b",
      "e4ab66733f6147a2b57ac31c56d823ec",
      "30a84bdde3d14754a947c2ef0e9c8c70",
      "b9982941845a4bdaaa44785cc0c47d99",
      "2febb3d1c4e549e1b8267b8d6338c371",
      "c57f26b447574eb29c3eea64b97e17bc",
      "63a0f9346d9949039d0295f348a7267b",
      "c1f6fce99edf4afc9f9c966cd4c604cd",
      "325cc34a752c4d9bb7268cf45a2aedad",
      "d78d2d71dc7044369b886de7f3c21392",
      "ad3c3618edfb405abd7902dfeeb3db25",
      "2988259da3704f289c3b07c1d1c4898a",
      "6b37e71b4cf54d5da45526abbd2463af",
      "901a52e89fb4475c98a0f6e2fc7c584a",
      "215b7b7bea9b4d43bdd1a7772c654743",
      "7bcb8c89b5ab40f2ab3aa65a192ffa75",
      "1eb66f94f0cf4ad3a54ce3e62a554e4c",
      "412f1863a5f34c7f8a7c5dcaf9e62d86",
      "be99440c7a37415fab31242197feca95",
      "a7f44403b3844c738dc911b88466bde5",
      "c8ccd2a6f07d4bab81d01fb58bd6795f",
      "d74f6970de1040a0ba4bc1df0dde35c7",
      "cf9a12e109564d2da0a8b09243dfe8e6",
      "bec0b3c84f714b40bb0a2a0ec6a043f0",
      "21c07e78e14948199ede3c94ff778b5e",
      "477808a8a6954d21890841f0f4c64e5f",
      "e1f3c7b5fc474ee8919c9c67e4598d6f",
      "1b1ede24dae64ceaa7753e4e20a78e08",
      "4b36cbae484e4d2fa4f6ab21af72f7fa",
      "86066a4c98fc41ee98ed343b950ef10b",
      "07c0ec88325f4dbb9616ae1fd18b8e41",
      "ccd72ac08c994443a3b646f16aa7fb3a",
      "b853b0e3f422438395351c7c1ab5825c",
      "ecbd484c37304dea8c8a411f8745d553",
      "6c65781e3f1b453a96ea8212b8388900",
      "bac6b9a704944e73b0252373c913e6c1",
      "743eca4be4b5401bbe89be51af0edf83",
      "193559fea7234aca8830fea1bfb90fa8",
      "58769ea2f36e4819aabf182bf548efc3",
      "04606349a5a747cd9ea34a67b90ee700",
      "e271011f3d16437d92626ee183201532",
      "8b68b8308a2845b5beb67784e2323a9d",
      "cbbb891cbd24435c9d2e4d487bc6b2c7",
      "0cb2fb5272ed438a8e6587be1cf9d894",
      "5a68309c6cab44c0966505a4019217ca",
      "afd312cfbb294777a5cc7484035f4166",
      "abb7818b1abd49f4b3c9c1c894712962",
      "bc4c232f441c4206b8f3e92690253a43",
      "680e0257519240e6869d1d649c04117a",
      "36801dd1e7db492092e57d67984e0e4b",
      "349e85012a55467f99c04b26aedaf1f9",
      "af7682f9f5474911ada44cac1972a15f",
      "6076da53e793481bb586c8c9b237a7b5",
      "a97fe80a30294254a123cfe3b288a682",
      "611c4e401c2f449a8e585b4529fcc292",
      "13b9f711cc1a4d658cca72a188f5aa29",
      "d946dfe7affc49c4889172b49dc37bcd",
      "0753e04edf9948aab77a09471450f207",
      "4a582eaf228b46009b5470a1c90580e0",
      "6f6fd9c33ef44a7abe17e0b02eb40415",
      "3d1d3778c2e843d2bebb0890bb7b9cee"
     ]
    },
    "id": "I43LbTpeqMiJ",
    "outputId": "adfeb842-11af-42f5-d188-1f8808833ad7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4762cc40db204d8b8664aef1e659d733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/475 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63a0f9346d9949039d0295f348a7267b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412f1863a5f34c7f8a7c5dcaf9e62d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b36cbae484e4d2fa4f6ab21af72f7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04606349a5a747cd9ea34a67b90ee700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349e85012a55467f99c04b26aedaf1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is the primary goal of Artificial Intelligence (AI) in computer systems?\n",
      "Answer: creating systems capable of performing tasks that normally require human intelligence\n",
      "Score: 0.6015933752059937\n",
      "\n",
      "Question: What is the capital of united states?\n",
      "Answer not found in the context.\n",
      "Model confidence score: 1.2177049299455511e-11\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load an open-source transformer QA model\n",
    "qa_model = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"ahotrod/electra_large_discriminator_squad2_512\"\n",
    ")\n",
    "\n",
    "# Your context\n",
    "context = \"\"\"\n",
    "Artificial Intelligence (AI) is a broad field of computer science that focuses on\n",
    "creating systems capable of performing tasks that normally require human intelligence.\n",
    "These tasks include understanding language, recognizing patterns, solving problems,\n",
    "making decisions, and learning from experience.\n",
    "AI aims to build machines that can mimic cognitive functions such as reasoning,\n",
    "perception, and decision-making.\n",
    "\"\"\"\n",
    "\n",
    "# Ask any questions from the paragraph\n",
    "question1 = \"What is the primary goal of Artificial Intelligence (AI) in computer systems?\"\n",
    "question2 = \"What is the capital of united states?\"\n",
    "\n",
    "def answer_question(question, context, threshold=0.6):\n",
    "\n",
    "    result = qa_model(question=question, context=context)\n",
    "\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "\n",
    "    # If model score is high enough -> answer exists in context\n",
    "    if result['score'] >= threshold:\n",
    "        print(\"Answer:\", result['answer'])\n",
    "        print(\"Score:\", result['score'])\n",
    "    else:\n",
    "        print(\"Answer not found in the context.\")\n",
    "        print(\"Model confidence score:\", result['score'])\n",
    "\n",
    "# Predict answers\n",
    "answer_question(question1, context)\n",
    "answer_question(question2, context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWo3nFnvxcsJ"
   },
   "source": [
    "###**First Question**\n",
    "The first question gets correctly predicted because the answer is present in the given paragraph. The model also returns a higher score compared to previous models, showing that the ELECTRA-based QA model understands the context more accurately.\n",
    "\n",
    "###**Second Question**\n",
    "The second question is correctly rejected since the answer (“What is the capital of United States?”) is not present in the context.\n",
    "The threshold-based check works properly, and instead of giving a wrong answer, the system prints “Answer not found in the context”, which makes the QA model more reliable for practical use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SVGtBF_yqNH0",
    "outputId": "a7d78ecb-8040-4537-e158-47d90ad08fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting playwright\n",
      "  Downloading playwright-1.57.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
      "Collecting pyee<14,>=13 (from playwright)\n",
      "  Downloading pyee-13.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from playwright) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from pyee<14,>=13->playwright) (4.15.0)\n",
      "Downloading playwright-1.57.0-py3-none-manylinux1_x86_64.whl (46.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyee-13.0.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: pyee, playwright\n",
      "Successfully installed playwright-1.57.0 pyee-13.0.0\n",
      "Downloading Chromium 143.0.7499.4 (playwright build v1200)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1200/chromium-linux.zip\u001b[22m\n",
      "(node:1591) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n",
      "(Use `node --trace-deprecation ...` to show where the warning was created)\n",
      "\u001b[1G164.7 MiB [] 0% 334.7s\u001b[0K\u001b[1G164.7 MiB [] 0% 43.3s\u001b[0K\u001b[1G164.7 MiB [] 0% 15.2s\u001b[0K\u001b[1G164.7 MiB [] 0% 13.3s\u001b[0K\u001b[1G164.7 MiB [] 0% 7.2s\u001b[0K\u001b[1G164.7 MiB [] 1% 4.3s\u001b[0K\u001b[1G164.7 MiB [] 2% 3.3s\u001b[0K\u001b[1G164.7 MiB [] 3% 2.9s\u001b[0K\u001b[1G164.7 MiB [] 4% 3.0s\u001b[0K\u001b[1G164.7 MiB [] 4% 2.9s\u001b[0K\u001b[1G164.7 MiB [] 5% 2.6s\u001b[0K\u001b[1G164.7 MiB [] 7% 2.4s\u001b[0K\u001b[1G164.7 MiB [] 7% 2.2s\u001b[0K\u001b[1G164.7 MiB [] 8% 2.2s\u001b[0K\u001b[1G164.7 MiB [] 9% 2.1s\u001b[0K\u001b[1G164.7 MiB [] 10% 2.0s\u001b[0K\u001b[1G164.7 MiB [] 11% 1.9s\u001b[0K\u001b[1G164.7 MiB [] 13% 1.8s\u001b[0K\u001b[1G164.7 MiB [] 14% 1.8s\u001b[0K\u001b[1G164.7 MiB [] 15% 1.7s\u001b[0K\u001b[1G164.7 MiB [] 16% 1.8s\u001b[0K\u001b[1G164.7 MiB [] 17% 1.8s\u001b[0K\u001b[1G164.7 MiB [] 18% 1.7s\u001b[0K\u001b[1G164.7 MiB [] 19% 1.7s\u001b[0K\u001b[1G164.7 MiB [] 20% 1.7s\u001b[0K\u001b[1G164.7 MiB [] 21% 1.6s\u001b[0K\u001b[1G164.7 MiB [] 22% 1.6s\u001b[0K\u001b[1G164.7 MiB [] 24% 1.5s\u001b[0K\u001b[1G164.7 MiB [] 25% 1.5s\u001b[0K\u001b[1G164.7 MiB [] 26% 1.4s\u001b[0K\u001b[1G164.7 MiB [] 27% 1.4s\u001b[0K\u001b[1G164.7 MiB [] 29% 1.3s\u001b[0K\u001b[1G164.7 MiB [] 30% 1.3s\u001b[0K\u001b[1G164.7 MiB [] 31% 1.2s\u001b[0K\u001b[1G164.7 MiB [] 32% 1.2s\u001b[0K\u001b[1G164.7 MiB [] 33% 1.2s\u001b[0K\u001b[1G164.7 MiB [] 34% 1.2s\u001b[0K\u001b[1G164.7 MiB [] 35% 1.2s\u001b[0K\u001b[1G164.7 MiB [] 37% 1.1s\u001b[0K\u001b[1G164.7 MiB [] 38% 1.1s\u001b[0K\u001b[1G164.7 MiB [] 39% 1.1s\u001b[0K\u001b[1G164.7 MiB [] 40% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 41% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 42% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 43% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 44% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 45% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 46% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 47% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 48% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 49% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 50% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 51% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 52% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 54% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 55% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 56% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 57% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 58% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 59% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 60% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 62% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 63% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 64% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 65% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 66% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 67% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 68% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 70% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 71% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 72% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 73% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 74% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 75% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 77% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 78% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 79% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 80% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 82% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 83% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 84% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 85% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 86% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 88% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 89% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 90% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 92% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 93% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 95% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 96% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 97% 0.0s\u001b[0K\u001b[1G164.7 MiB [] 98% 0.0s\u001b[0K\u001b[1G164.7 MiB [] 100% 0.0s\u001b[0K\n",
      "Chromium 143.0.7499.4 (playwright build v1200) downloaded to /root/.cache/ms-playwright/chromium-1200\n",
      "Downloading FFMPEG playwright build v1011\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/ffmpeg/1011/ffmpeg-linux.zip\u001b[22m\n",
      "(node:1638) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n",
      "(Use `node --trace-deprecation ...` to show where the warning was created)\n",
      "\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 2% 1.0s\u001b[0K\u001b[1G2.3 MiB [] 5% 0.8s\u001b[0K\u001b[1G2.3 MiB [] 12% 0.5s\u001b[0K\u001b[1G2.3 MiB [] 25% 0.3s\u001b[0K\u001b[1G2.3 MiB [] 36% 0.2s\u001b[0K\u001b[1G2.3 MiB [] 53% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 90% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n",
      "FFMPEG playwright build v1011 downloaded to /root/.cache/ms-playwright/ffmpeg-1011\n",
      "Downloading Chromium Headless Shell 143.0.7499.4 (playwright build v1200)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1200/chromium-headless-shell-linux.zip\u001b[22m\n",
      "(node:1653) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n",
      "(Use `node --trace-deprecation ...` to show where the warning was created)\n",
      "\u001b[1G109.7 MiB [] 0% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 0% 39.0s\u001b[0K\u001b[1G109.7 MiB [] 0% 14.2s\u001b[0K\u001b[1G109.7 MiB [] 0% 7.0s\u001b[0K\u001b[1G109.7 MiB [] 2% 3.5s\u001b[0K\u001b[1G109.7 MiB [] 3% 2.5s\u001b[0K\u001b[1G109.7 MiB [] 4% 2.0s\u001b[0K\u001b[1G109.7 MiB [] 6% 2.0s\u001b[0K\u001b[1G109.7 MiB [] 7% 1.9s\u001b[0K\u001b[1G109.7 MiB [] 8% 1.7s\u001b[0K\u001b[1G109.7 MiB [] 10% 1.5s\u001b[0K\u001b[1G109.7 MiB [] 11% 1.4s\u001b[0K\u001b[1G109.7 MiB [] 13% 1.4s\u001b[0K\u001b[1G109.7 MiB [] 14% 1.3s\u001b[0K\u001b[1G109.7 MiB [] 15% 1.3s\u001b[0K\u001b[1G109.7 MiB [] 17% 1.2s\u001b[0K\u001b[1G109.7 MiB [] 19% 1.2s\u001b[0K\u001b[1G109.7 MiB [] 20% 1.1s\u001b[0K\u001b[1G109.7 MiB [] 22% 1.1s\u001b[0K\u001b[1G109.7 MiB [] 23% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 24% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 26% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 28% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 29% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 31% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 32% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 34% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 36% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 38% 0.7s\u001b[0K\u001b[1G109.7 MiB [] 40% 0.7s\u001b[0K\u001b[1G109.7 MiB [] 42% 0.7s\u001b[0K\u001b[1G109.7 MiB [] 44% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 46% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 48% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 50% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 52% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 54% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 55% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 56% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 57% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 58% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 59% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 61% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 62% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 63% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 64% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 66% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 68% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 69% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 70% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 71% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 73% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 75% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 77% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 78% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 80% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 82% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 83% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 85% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 87% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 88% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 90% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 91% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 93% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 95% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 96% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 98% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 100% 0.0s\u001b[0K\n",
      "Chromium Headless Shell 143.0.7499.4 (playwright build v1200) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1200\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  at-spi2-core gsettings-desktop-schemas libatk1.0-data libxtst6\n",
      "  session-migration\n",
      "The following NEW packages will be installed:\n",
      "  at-spi2-core gsettings-desktop-schemas libatk-bridge2.0-0 libatk1.0-0\n",
      "  libatk1.0-data libatspi2.0-0 libxcomposite1 libxtst6 session-migration\n",
      "0 upgraded, 9 newly installed, 0 to remove and 1 not upgraded.\n",
      "Need to get 318 kB of archives.\n",
      "After this operation, 1,497 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatspi2.0-0 amd64 2.44.0-3 [80.9 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9,774 B]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 at-spi2-core amd64 2.44.0-3 [54.4 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-data all 2.36.0-3build1 [2,824 B]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-0 amd64 2.36.0-3build1 [51.9 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-bridge2.0-0 amd64 2.38.0-3 [66.6 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7,192 B]\n",
      "Fetched 318 kB in 1s (226 kB/s)\n",
      "Selecting previously unselected package libatspi2.0-0:amd64.\n",
      "(Reading database ... 117528 files and directories currently installed.)\n",
      "Preparing to unpack .../0-libatspi2.0-0_2.44.0-3_amd64.deb ...\n",
      "Unpacking libatspi2.0-0:amd64 (2.44.0-3) ...\n",
      "Selecting previously unselected package libxtst6:amd64.\n",
      "Preparing to unpack .../1-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
      "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
      "Selecting previously unselected package session-migration.\n",
      "Preparing to unpack .../2-session-migration_0.3.6_amd64.deb ...\n",
      "Unpacking session-migration (0.3.6) ...\n",
      "Selecting previously unselected package gsettings-desktop-schemas.\n",
      "Preparing to unpack .../3-gsettings-desktop-schemas_42.0-1ubuntu1_all.deb ...\n",
      "Unpacking gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
      "Selecting previously unselected package at-spi2-core.\n",
      "Preparing to unpack .../4-at-spi2-core_2.44.0-3_amd64.deb ...\n",
      "Unpacking at-spi2-core (2.44.0-3) ...\n",
      "Selecting previously unselected package libatk1.0-data.\n",
      "Preparing to unpack .../5-libatk1.0-data_2.36.0-3build1_all.deb ...\n",
      "Unpacking libatk1.0-data (2.36.0-3build1) ...\n",
      "Selecting previously unselected package libatk1.0-0:amd64.\n",
      "Preparing to unpack .../6-libatk1.0-0_2.36.0-3build1_amd64.deb ...\n",
      "Unpacking libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
      "Selecting previously unselected package libatk-bridge2.0-0:amd64.\n",
      "Preparing to unpack .../7-libatk-bridge2.0-0_2.38.0-3_amd64.deb ...\n",
      "Unpacking libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
      "Selecting previously unselected package libxcomposite1:amd64.\n",
      "Preparing to unpack .../8-libxcomposite1_1%3a0.4.5-1build2_amd64.deb ...\n",
      "Unpacking libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
      "Setting up session-migration (0.3.6) ...\n",
      "Created symlink /etc/systemd/user/graphical-session-pre.target.wants/session-migration.service → /usr/lib/systemd/user/session-migration.service.\n",
      "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
      "Setting up libatspi2.0-0:amd64 (2.44.0-3) ...\n",
      "Setting up libatk1.0-data (2.36.0-3build1) ...\n",
      "Setting up libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
      "Setting up libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
      "Setting up gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
      "Setting up libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
      "Processing triggers for libglib2.0-0:amd64 (2.72.4-0ubuntu2.6) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.11) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
      "\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Setting up at-spi2-core (2.44.0-3) ...\n",
      "Collected 6 products\n",
      "Saved CSV → output/products_ajax.csv\n",
      "Saved JSON → output/products_ajax.json\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "\n",
    "!pip install playwright nest_asyncio\n",
    "!playwright install chromium\n",
    "!apt-get install libatk1.0-0 libatk-bridge2.0-0 libatspi2.0-0 libxcomposite1\n",
    "\n",
    "# Common imports and Colab event loop fix\n",
    "import asyncio, json, csv, time\n",
    "from pathlib import Path\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "#AJAX_URL = \"https://webscraper.io/test-sites/e-commerce/ajax\"  # target page\n",
    "#AJAX_URL = \"https://webscraper.io/test-sites\"\n",
    "AJAX_URL = \"https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=1\"\n",
    "async def scrape_ajax_site():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)   # set False to watch it\n",
    "        ctx = await browser.new_context()\n",
    "        page = await ctx.new_page()\n",
    "        await page.goto(AJAX_URL, timeout=60000)\n",
    "\n",
    "        # Wait for initial product grid\n",
    "        await page.wait_for_selector(\".thumbnail\", timeout=30000)\n",
    "\n",
    "        # Keep clicking \"Load more\" until the button disappears or is disabled\n",
    "        while True:\n",
    "            try:\n",
    "                # Button sometimes labeled \"Load more\" or has class .btn-load-more\n",
    "                load_more = await page.query_selector(\"button:has-text('Load more')\")\n",
    "                if not load_more:\n",
    "                    break\n",
    "                is_disabled = await load_more.get_attribute(\"disabled\")\n",
    "                if is_disabled:\n",
    "                    break\n",
    "\n",
    "                # Click and wait for new items to appear\n",
    "                before_count = len(await page.query_selector_all(\".thumbnail\"))\n",
    "                await load_more.click()\n",
    "                # Wait for network idle and more thumbnails to load\n",
    "                await page.wait_for_load_state(\"networkidle\")\n",
    "                # Soft wait: poll until count increases or timeout\n",
    "                for _ in range(30):\n",
    "                    after_count = len(await page.query_selector_all(\".thumbnail\"))\n",
    "                    if after_count > before_count:\n",
    "                        break\n",
    "                    await asyncio.sleep(0.2)\n",
    "                # If no increase, assume end\n",
    "                if after_count <= before_count:\n",
    "                    break\n",
    "\n",
    "            except Exception:\n",
    "                # If the button isn't present or any error occurs, exit the loop\n",
    "                break\n",
    "\n",
    "        # Extract product cards\n",
    "        cards = await page.query_selector_all(\".thumbnail\")\n",
    "        rows = []\n",
    "\n",
    "        for card in cards:\n",
    "            # Title text and product link\n",
    "            title_el = await card.query_selector(\".title\")\n",
    "            title = (await title_el.text_content()).strip() if title_el else None\n",
    "            url = await title_el.get_attribute(\"href\") if title_el else None\n",
    "            # Price\n",
    "            price_el = await card.query_selector(\".price\")\n",
    "            price = (await price_el.text_content()).strip() if price_el else None\n",
    "            # Rating (count of .glyphicon-star)\n",
    "            stars = await card.query_selector_all(\".ratings .glyphicon-star\")\n",
    "            rating = len(stars) if stars else 0\n",
    "            # Image\n",
    "            img_el = await card.query_selector(\"img\")\n",
    "            img_src = await img_el.get_attribute(\"src\") if img_el else None\n",
    "\n",
    "            rows.append({\n",
    "                \"title\": title,\n",
    "                \"price\": price,\n",
    "                \"rating_stars\": rating,\n",
    "                \"product_url\": url,\n",
    "                \"image_url\": img_src\n",
    "            })\n",
    "\n",
    "        await browser.close()\n",
    "        return rows\n",
    "\n",
    "# Run and save\n",
    "data = asyncio.get_event_loop().run_until_complete(scrape_ajax_site())\n",
    "print(f\"Collected {len(data)} products\")\n",
    "\n",
    "# Save to CSV/JSON\n",
    "Path(\"output\").mkdir(exist_ok=True)\n",
    "csv_path = Path(\"output/products_ajax.csv\")\n",
    "json_path = Path(\"output/products_ajax.json\")\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved CSV → {csv_path}\")\n",
    "print(f\"Saved JSON → {json_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QloIZXvP2HbA"
   },
   "source": [
    "##**Observations**\n",
    "\n",
    "These cells continue the Playwright workflow by automating navigation, loading pages and interacting with content. The scripts demonstrate how automated browsing can fetch structured information. The main observation here is that the automation pipeline is configured successfully and web-driven data collection becomes possible, enabling downstream analysis tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oA76COEKwwnj",
    "outputId": "71b8b0f4-0b2b-4ee8-ae36-29d22b0b983b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: playwright in /usr/local/lib/python3.12/dist-packages (1.57.0)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
      "Requirement already satisfied: pyee<14,>=13 in /usr/local/lib/python3.12/dist-packages (from playwright) (13.0.0)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from playwright) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from pyee<14,>=13->playwright) (4.15.0)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "libatk-bridge2.0-0 is already the newest version (2.38.0-3).\n",
      "libatk1.0-0 is already the newest version (2.36.0-3build1).\n",
      "libatspi2.0-0 is already the newest version (2.44.0-3).\n",
      "libxcomposite1 is already the newest version (1:0.4.5-1build2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.\n",
      "Collected 6 products\n",
      "Saved CSV → output/products_tablets.csv\n",
      "Saved JSON → output/products_tablets.json\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "\n",
    "!pip install playwright nest_asyncio\n",
    "!playwright install chromium\n",
    "!apt-get install libatk1.0-0 libatk-bridge2.0-0 libatspi2.0-0 libxcomposite1\n",
    "\n",
    "# Common imports and Colab event loop fix\n",
    "import asyncio, json, csv, time\n",
    "from pathlib import Path\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# 💡 MODIFIED TARGET URL FOR TABLETS\n",
    "# The original URL was for Laptops: \"https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=1\"\n",
    "# The new URL is for Tablets:\n",
    "AJAX_URL = \"https://webscraper.io/test-sites/e-commerce/static/computers/tablets?page=1\"\n",
    "\n",
    "async def scrape_ajax_site():\n",
    "    async with async_playwright() as p:\n",
    "        # Launch browser\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        ctx = await browser.new_context()\n",
    "        page = await ctx.new_page()\n",
    "        await page.goto(AJAX_URL, timeout=60000)\n",
    "\n",
    "        # Wait for initial product grid\n",
    "        await page.wait_for_selector(\".thumbnail\", timeout=30000)\n",
    "\n",
    "        # Keep clicking \"Load more\" until the button disappears or is disabled\n",
    "        while True:\n",
    "            try:\n",
    "                # Button sometimes labeled \"Load more\" or has class .btn-load-more\n",
    "                load_more = await page.query_selector(\"button:has-text('Load more')\")\n",
    "                if not load_more:\n",
    "                    break\n",
    "                is_disabled = await load_more.get_attribute(\"disabled\")\n",
    "                if is_disabled:\n",
    "                    break\n",
    "\n",
    "                # Click and wait for new items to appear\n",
    "                before_count = len(await page.query_selector_all(\".thumbnail\"))\n",
    "                await load_more.click()\n",
    "                # Wait for network idle and more thumbnails to load\n",
    "                await page.wait_for_load_state(\"networkidle\")\n",
    "                # Soft wait: poll until count increases or timeout\n",
    "                for _ in range(30):\n",
    "                    after_count = len(await page.query_selector_all(\".thumbnail\"))\n",
    "                    if after_count > before_count:\n",
    "                        break\n",
    "                    await asyncio.sleep(0.2)\n",
    "                # If no increase, assume end\n",
    "                if after_count <= before_count:\n",
    "                    break\n",
    "\n",
    "            except Exception:\n",
    "                # If the button isn't present or any error occurs, exit the loop\n",
    "                break\n",
    "\n",
    "        # Extract product cards\n",
    "        cards = await page.query_selector_all(\".thumbnail\")\n",
    "        rows = []\n",
    "\n",
    "        for card in cards:\n",
    "            # Title text and product link\n",
    "            title_el = await card.query_selector(\".title\")\n",
    "            title = (await title_el.text_content()).strip() if title_el else None\n",
    "            # Note: The URL is relative, you may need to prefix it with the base domain if linking externally\n",
    "            url = await title_el.get_attribute(\"href\") if title_el else None\n",
    "            # Price\n",
    "            price_el = await card.query_selector(\".price\")\n",
    "            price = (await price_el.text_content()).strip() if price_el else None\n",
    "            # Rating (count of .glyphicon-star)\n",
    "            stars = await card.query_selector_all(\".ratings .glyphicon-star\")\n",
    "            rating = len(stars) if stars else 0\n",
    "            # Image\n",
    "            img_el = await card.query_selector(\"img\")\n",
    "            img_src = await img_el.get_attribute(\"src\") if img_el else None\n",
    "\n",
    "            rows.append({\n",
    "                \"title\": title,\n",
    "                \"price\": price,\n",
    "                \"rating_stars\": rating,\n",
    "                \"product_url\": url,\n",
    "                \"image_url\": img_src\n",
    "            })\n",
    "\n",
    "        await browser.close()\n",
    "        return rows\n",
    "\n",
    "# Run and save\n",
    "data = asyncio.get_event_loop().run_until_complete(scrape_ajax_site())\n",
    "print(f\"Collected {len(data)} products\")\n",
    "\n",
    "# Save to CSV/JSON\n",
    "Path(\"output\").mkdir(exist_ok=True)\n",
    "csv_path = Path(\"output/products_tablets.csv\") # Renamed output file for clarity\n",
    "json_path = Path(\"output/products_tablets.json\") # Renamed output file for clarity\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    # Ensure data is not empty before attempting to write header\n",
    "    if data:\n",
    "        writer = csv.DictWriter(f, fieldnames=data[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved CSV → {csv_path}\")\n",
    "print(f\"Saved JSON → {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8ng7DA42BaX"
   },
   "source": [
    "##**Observations**\n",
    "\n",
    "This cell installs libraries such as feedparser and vaderSentiment to support RSS-based news extraction and sentiment analysis. This prepares the environment to fetch external news data and derive polarity scores that later feed into pricing and trend logic. Again, this step is infrastructural rather than analytical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FVK6BTEIPzoK",
    "outputId": "0b07ddbd-40de-49f2-ee08-5d260b3a2c51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: playwright in /usr/local/lib/python3.12/dist-packages (1.57.0)\n",
      "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
      "Requirement already satisfied: pyee<14,>=13 in /usr/local/lib/python3.12/dist-packages (from playwright) (13.0.0)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from playwright) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from pyee<14,>=13->playwright) (4.15.0)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "libatk-bridge2.0-0 is already the newest version (2.38.0-3).\n",
      "libatk1.0-0 is already the newest version (2.36.0-3build1).\n",
      "libatspi2.0-0 is already the newest version (2.44.0-3).\n",
      "libxcomposite1 is already the newest version (1:0.4.5-1build2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.\n",
      "Starting concurrent scraping tasks...\n",
      "[LAPTOPS] -> Attempting to scrape page 1 at: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=1\n",
      "[TABLETS] -> Attempting to scrape page 1 at: https://webscraper.io/test-sites/e-commerce/static/computers/tablets?page=1\n",
      "[PHONES] -> Attempting to scrape page 1 at: https://webscraper.io/test-sites/e-commerce/static/phones/touch\n",
      "   ✅ [TABLETS] Collected 6 products from page 1.\n",
      "   ✅ [PHONES] Collected 6 products from page 1.\n",
      "   ✅ [LAPTOPS] Collected 6 products from page 1.\n",
      "[TABLETS] -> Attempting to scrape page 2 at: https://webscraper.io/test-sites/e-commerce/static/computers/tablets?page=2\n",
      "[LAPTOPS] -> Attempting to scrape page 2 at: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=2\n",
      "   ✅ [TABLETS] Collected 6 products from page 2.\n",
      "[TABLETS] -> Attempting to scrape page 3 at: https://webscraper.io/test-sites/e-commerce/static/computers/tablets?page=3\n",
      "   ✅ [LAPTOPS] Collected 6 products from page 2.\n",
      "[LAPTOPS] -> Attempting to scrape page 3 at: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=3\n",
      "   ✅ [TABLETS] Collected 6 products from page 3.\n",
      "[TABLETS] -> Attempting to scrape page 4 at: https://webscraper.io/test-sites/e-commerce/static/computers/tablets?page=4\n",
      "   ✅ [LAPTOPS] Collected 6 products from page 3.\n",
      "[LAPTOPS] -> Attempting to scrape page 4 at: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=4\n",
      "   ✅ [TABLETS] Collected 3 products from page 4.\n",
      "[TABLETS] -> Attempting to scrape page 5 at: https://webscraper.io/test-sites/e-commerce/static/computers/tablets?page=5\n",
      "   ✅ [LAPTOPS] Collected 6 products from page 4.\n",
      "[LAPTOPS] -> Attempting to scrape page 5 at: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=5\n",
      "   ✅ [LAPTOPS] Collected 6 products from page 5.\n",
      "[LAPTOPS] -> Attempting to scrape page 6 at: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=6\n",
      "   ✅ [LAPTOPS] Collected 6 products from page 6.\n",
      "[LAPTOPS] -> Attempting to scrape page 7 at: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=7\n",
      "[TABLETS] No products or expected elements found on page 5. Stopping.\n",
      "   ✅ [LAPTOPS] Collected 6 products from page 7.\n",
      "[LAPTOPS] -> Attempting to scrape page 8 at: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=8\n",
      "   ✅ [LAPTOPS] Collected 6 products from page 8.\n",
      "[LAPTOPS] -> Attempting to scrape page 9 at: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=9\n",
      "   ✅ [LAPTOPS] Collected 6 products from page 9.\n",
      "[LAPTOPS] -> Attempting to scrape page 10 at: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=10\n",
      "   ✅ [LAPTOPS] Collected 6 products from page 10.\n",
      "[LAPTOPS] -> Attempting to scrape page 11 at: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=11\n",
      "   ✅ [LAPTOPS] Collected 6 products from page 11.\n",
      "[LAPTOPS] -> Attempting to scrape page 12 at: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=12\n",
      "   ✅ [LAPTOPS] Collected 6 products from page 12.\n",
      "[LAPTOPS] -> Attempting to scrape page 13 at: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=13\n",
      "   ✅ [LAPTOPS] Collected 6 products from page 13.\n",
      "[LAPTOPS] -> Attempting to scrape page 14 at: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=14\n",
      "   ✅ [LAPTOPS] Collected 6 products from page 14.\n",
      "[LAPTOPS] -> Attempting to scrape page 15 at: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=15\n",
      "   ✅ [LAPTOPS] Collected 6 products from page 15.\n",
      "[LAPTOPS] -> Attempting to scrape page 16 at: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=16\n",
      "   ✅ [LAPTOPS] Collected 6 products from page 16.\n",
      "[LAPTOPS] -> Attempting to scrape page 17 at: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=17\n",
      "   ✅ [LAPTOPS] Collected 6 products from page 17.\n",
      "[LAPTOPS] -> Attempting to scrape page 18 at: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=18\n",
      "   ✅ [LAPTOPS] Collected 6 products from page 18.\n",
      "[LAPTOPS] -> Attempting to scrape page 19 at: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=19\n",
      "   ✅ [LAPTOPS] Collected 6 products from page 19.\n",
      "[LAPTOPS] -> Attempting to scrape page 20 at: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=20\n",
      "   ✅ [LAPTOPS] Collected 3 products from page 20.\n",
      "[LAPTOPS] -> Attempting to scrape page 21 at: https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page=21\n",
      "[LAPTOPS] No products or expected elements found on page 21. Stopping.\n",
      "All concurrent tasks finished.\n",
      "\n",
      "--- SCRAPING COMPLETE ---\n",
      "Collected a TOTAL of 144 products from 3 categories.\n",
      "Saved CSV → output/products_all_categories_networkidle.csv\n",
      "Saved JSON → output/products_all_categories_networkidle.json\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "\n",
    "!pip install playwright nest_asyncio\n",
    "!playwright install chromium\n",
    "!apt-get install libatk1.0-0 libatk-bridge2.0-0 libatspi2.0-0 libxcomposite1\n",
    "\n",
    "# Common imports and Colab event loop fix\n",
    "import asyncio, json, csv, time\n",
    "from pathlib import Path\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "# Import Playwright's specific exceptions\n",
    "from playwright.async_api import async_playwright, TimeoutError\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "# Define the categories and their base URLs\n",
    "CATEGORIES = {\n",
    "    \"laptops\": \"https://webscraper.io/test-sites/e-commerce/static/computers/laptops?page={}\",\n",
    "    \"tablets\": \"https://webscraper.io/test-sites/e-commerce/static/computers/tablets?page={}\",\n",
    "    \"phones\": \"https://webscraper.io/test-sites/e-commerce/static/phones/touch\",\n",
    "}\n",
    "\n",
    "# --- SCRAPING LOGIC ---\n",
    "\n",
    "async def scrape_category(category_name, base_url, browser_context):\n",
    "    \"\"\"Scrapes all pages for a single category, using networkidle for loading.\"\"\"\n",
    "\n",
    "    category_products = []\n",
    "    current_page = 1\n",
    "\n",
    "    is_paginated = \"{}\" in base_url\n",
    "\n",
    "    while True:\n",
    "        # 1. Construct the URL for the current page\n",
    "        current_url = base_url.format(current_page) if is_paginated else base_url\n",
    "\n",
    "        print(f\"[{category_name.upper()}] -> Attempting to scrape page {current_page} at: {current_url}\")\n",
    "\n",
    "        page = await browser_context.new_page()\n",
    "\n",
    "        # 💡 KEY CHANGE: Using wait_until=\"networkidle\" instead of a fixed timeout value.\n",
    "        try:\n",
    "            # Playwright waits until there are no more than 0 or 1 network connections\n",
    "            # for at least 500ms, indicating loading is complete.\n",
    "            await page.goto(current_url, wait_until=\"networkidle\")\n",
    "        except TimeoutError:\n",
    "            # Note: page.goto() still has a default 30s timeout, which will be triggered\n",
    "            # if the network never settles.\n",
    "            print(f\"🚨 [{category_name.upper()}] FAILED: Navigation timed out while waiting for networkidle. Stopping category.\")\n",
    "            await page.close()\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"🚨 [{category_name.upper()}] FAILED: Could not navigate. Error: {e}. Stopping category.\")\n",
    "            await page.close()\n",
    "            break\n",
    "\n",
    "        # Wait for the product grid element to appear (still needs a timeout)\n",
    "        try:\n",
    "            await page.wait_for_selector(\".thumbnail\", timeout=10000)\n",
    "        except Exception:\n",
    "            print(f\"[{category_name.upper()}] No products or expected elements found on page {current_page}. Stopping.\")\n",
    "            await page.close()\n",
    "            break\n",
    "\n",
    "        # Check if there are any products on the page (Stop Condition)\n",
    "        cards = await page.query_selector_all(\".thumbnail\")\n",
    "        if not cards:\n",
    "            print(f\"[{category_name.upper()}] No products found on page {current_page}. Stopping.\")\n",
    "            await page.close()\n",
    "            break\n",
    "\n",
    "        # 2. Extract product cards (The rest of the logic remains the same)\n",
    "        page_products = []\n",
    "        for card in cards:\n",
    "            title_el = await card.query_selector(\".title\")\n",
    "            title = (await title_el.text_content()).strip() if title_el else None\n",
    "            url = await title_el.get_attribute(\"href\") if title_el else None\n",
    "            price_el = await card.query_selector(\".price\")\n",
    "            price = (await price_el.text_content()).strip() if price_el else None\n",
    "            stars = await card.query_selector_all(\".ratings .glyphicon-star\")\n",
    "            rating = len(stars) if stars else 0\n",
    "            img_el = await card.query_selector(\"img\")\n",
    "            img_src = await img_el.get_attribute(\"src\") if img_el else None\n",
    "\n",
    "            page_products.append({\n",
    "                \"category\": category_name,\n",
    "                \"title\": title,\n",
    "                \"price\": price,\n",
    "                \"rating_stars\": rating,\n",
    "                \"product_url\": url,\n",
    "                \"image_url\": img_src\n",
    "            })\n",
    "\n",
    "        print(f\"   ✅ [{category_name.upper()}] Collected {len(page_products)} products from page {current_page}.\")\n",
    "        category_products.extend(page_products)\n",
    "\n",
    "        await page.close()\n",
    "\n",
    "        # 3. Check for next page\n",
    "        if not is_paginated:\n",
    "            break\n",
    "\n",
    "        current_page += 1\n",
    "\n",
    "    return category_products\n",
    "\n",
    "# --- MAIN EXECUTION (No changes needed here) ---\n",
    "\n",
    "async def main_scraper():\n",
    "    \"\"\"Manages concurrent scraping of all categories.\"\"\"\n",
    "    all_products = []\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        ctx = await browser.new_context()\n",
    "\n",
    "        tasks = []\n",
    "        for name, url in CATEGORIES.items():\n",
    "            tasks.append(scrape_category(name, url, ctx))\n",
    "\n",
    "        print(\"Starting concurrent scraping tasks...\")\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        print(\"All concurrent tasks finished.\")\n",
    "\n",
    "        for result in results:\n",
    "            all_products.extend(result)\n",
    "\n",
    "        await browser.close()\n",
    "        return all_products\n",
    "\n",
    "# Run and save\n",
    "data = asyncio.get_event_loop().run_until_complete(main_scraper())\n",
    "print(f\"\\n--- SCRAPING COMPLETE ---\")\n",
    "print(f\"Collected a TOTAL of {len(data)} products from {len(CATEGORIES)} categories.\")\n",
    "\n",
    "# Save to CSV/JSON\n",
    "Path(\"output\").mkdir(exist_ok=True)\n",
    "csv_path = Path(\"output/products_all_categories_networkidle.csv\")\n",
    "json_path = Path(\"output/products_all_categories_networkidle.json\")\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    if data:\n",
    "        fieldnames = [\"category\"] + [k for k in data[0].keys() if k != \"category\"]\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved CSV → {csv_path}\")\n",
    "print(f\"Saved JSON → {json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LrFzNess12x6"
   },
   "source": [
    "##**Observations (Milestone - 2)**\n",
    "\n",
    "This major cell builds a complete AI-assisted book pricing system. It scrapes book details such as title, price, rating, stock and description, enhances the data using Google Books API reviews and author information, and performs simple keyword-based sentiment analysis on book descriptions. It also fetches live publishing-related news and computes a market-trend score, while Jaccard similarity is used to measure how closely a book’s themes align with trending topics. These signals — along with stock levels, demand proxies and author popularity — are combined into a pricing multiplier. Books with strong signals (high ratings, positive sentiment, trending topics, low stock, high demand) have their prices increased, while weaker titles receive discounted adjustments. Each price decision is printed transparently along with the reasoning, and the results are stored in a CSV file for later review."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
